{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model import Stage2Model, FaceModel, SelectNet_resnet, SelectNet\n",
    "from helper_funcs import affine_crop, stage2_pred_softmax, calc_centroid, affine_mapback\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class ModelEnd2End(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelEnd2End, self).__init__()\n",
    "        self.modelA = FaceModel()\n",
    "        self.stn_model = SelectNet_resnet()\n",
    "        self.modelC = Stage2Model()\n",
    "        self.modelA.eval()\n",
    "        self.stn_model.eval()\n",
    "        self.modelC.eval()\n",
    "\n",
    "    def forward(self, x, orig, orig_label=None):\n",
    "        N, L, H, W = orig.shape\n",
    "        stage1_pred = F.softmax(self.modelA(x), dim=1)\n",
    "        assert stage1_pred.shape == (N, 9, 128, 128)\n",
    "        theta = self.stn_model(stage1_pred)\n",
    "        # List: [5x[torch.size(N, 2, 81, 81)], 1x [torch.size(N, 4, 81, 81)]]\n",
    "        if orig_label is not None:\n",
    "            parts, parts_labels, _ = affine_crop(orig, orig_label, theta_in=theta, map_location=x.device)\n",
    "            # 如果没切到眉毛，就将切块位置换成对应眼睛\n",
    "            # lbrow\n",
    "            if(parts_labels[0].argmax(dim=1).sum() == 0):\n",
    "                parts[:, 0] = parts[:, 2]\n",
    "                parts_labels[0] = parts_labels[2]\n",
    "            # rbrow\n",
    "            if(parts_labels[1].argmax(dim=1).sum() == 0):\n",
    "                parts[:, 1] = parts[:, 3]\n",
    "                parts_labels[1] = parts_labels[3]\n",
    "        else:  \n",
    "            parts, _ = affine_crop(orig, label=None, theta_in=theta, map_location=x.device)\n",
    "        stage2_pred = self.modelC(parts)\n",
    "        softmax_stage2 = stage2_pred_softmax(stage2_pred) \n",
    "        final_pred = affine_mapback(softmax_stage2, theta, x.device)\n",
    "        return final_pred, parts, softmax_stage2\n",
    "                \n",
    "\n",
    "    def load_pretrain(self, path, device):\n",
    "        if len(path) == 0:\n",
    "            print(\"ERROR! No state path!\")\n",
    "            raise RuntimeError\n",
    "        elif len(path) == 1:\n",
    "            path = path[0]\n",
    "            print(\"load from\" + path)\n",
    "            state = torch.load(path, map_location=device)\n",
    "            self.modelA.load_state_dict(state['model1'])\n",
    "            self.stn_model.load_state_dict(state['select_net'])\n",
    "            self.modelC.load_state_dict(state['model2'])\n",
    "            self.modelA.eval()\n",
    "            self.stn_model.eval()\n",
    "            self.modelC.eval()\n",
    "        elif len(path) == 2:\n",
    "            # AB, C\n",
    "            pathAB, pathC = path\n",
    "            print(\"load from\" + pathAB)\n",
    "            print(\"load from\" + pathC)\n",
    "            stateAB = torch.load(pathAB, map_location=device)\n",
    "            stateC = torch.load(pathC, map_location=device)\n",
    "            self.modelA.load_state_dict(stateAB['model1'])\n",
    "            self.stn_model.load_state_dict(stateAB['select_net'])\n",
    "            self.modelC.load_state_dict(stateC['model2'])\n",
    "            self.modelA.eval()\n",
    "            self.stn_model.eval()\n",
    "            self.modelC.eval()\n",
    "        elif len(path) == 3:\n",
    "            # A, B, C\n",
    "            pathA, pathB, pathC = path\n",
    "            print(\"load from\" + pathA)\n",
    "            print(\"load from\" + pathB)\n",
    "            print(\"load from\" + pathC)\n",
    "            stateA = torch.load(pathA, map_location=device)\n",
    "            stateB = torch.load(pathB, map_location=device)\n",
    "            stateC = torch.load(pathC, map_location=device)\n",
    "            self.modelA.load_state_dict(stateA['model1'])\n",
    "            self.stn_model.load_state_dict(stateB['select_net'])\n",
    "            self.modelC.load_state_dict(stateC['model2'])\n",
    "            self.modelA.eval()\n",
    "            self.stn_model.eval()\n",
    "            self.modelC.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= ModelEnd2End()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from/home/yinzi/data4/STN-iCNN/checkpoints_ABC/ea0ac45c-0/best.pth.tar\n"
     ]
    }
   ],
   "source": [
    "path_ABC = os.path.join(\"/home/yinzi/data4/STN-iCNN/checkpoints_ABC/ea0ac45c-0\", 'best.pth.tar')\n",
    "model.eval()\n",
    "model.load_pretrain(path=[path_ABC], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  python3 3_end2end_tunning_all.py --batch_size 32 --cuda 8 --select_net 1 --pretrainA 1 --pretrainB 1 --pretrainC 0 --lr 0 --lr2 0.0025 --lr_s 0 --epoch 3000 --f1_eval 1\n",
    "# ea0ac45c-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import HelenDataset\n",
    "from torchvision import transforms\n",
    "from preprocess import ToPILImage, ToTensor, OrigPad, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "testDataset = HelenDataset(txt_file='testing.txt',\n",
    "                           root_dir=\"/data1/yinzi/datas\",\n",
    "                           parts_root_dir=\"/home/yinzi/data3/recroped_parts\",\n",
    "                           transform=  transforms.Compose([\n",
    "                                    ToPILImage(),\n",
    "                                    Resize((128, 128)),\n",
    "                                    ToTensor(),\n",
    "                                    OrigPad()\n",
    "                                ])\n",
    "                           )\n",
    "dataloader = DataLoader(testDataset, batch_size=1,\n",
    "                            shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 overall 为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9000639554324783\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn.functional as F\n",
    "from helper_funcs import F1Accuracy\n",
    "f1 = F1Accuracy(num=9)\n",
    "# f1_local = F1Accuracy(num=9)\n",
    "model.eval()\n",
    "for iter,batch in enumerate(dataloader):\n",
    "    image, labels, orig, orig_label = batch['image'], batch['labels'], batch['orig'], batch['orig_label']\n",
    "    pred, parts, stage2_pred =model(image, orig=orig, orig_label=None)\n",
    "    pred_arg = pred.argmax(dim=1, keepdim=False).detach()\n",
    "#     f1_local.collect(pred_arg, orig_label.argmax(dim=1, keepdim=False))\n",
    "#     f1_now = f1_local.calc()\n",
    "#     print(f\"f1 score of {iter} is: {f1_now}\")\n",
    "#         for i in range(6):\n",
    "#             plt.imshow(parts[0][i].permute(1,2,0).detach().cpu())\n",
    "#             plt.pause(0.01)\n",
    "#             plt.imshow(stage2_pred[i].argmax(dim=1, keepdim=False)[0].detach().cpu())\n",
    "#             plt.pause(0.01)\n",
    "#         plt.imshow(pred_arg[0])\n",
    "#         plt.pause(0.01)\n",
    "#         plt.imshow(orig_label.argmax(dim=1, keepdim=False)[0])\n",
    "#         plt.pause(0.01)\n",
    "    f1.collect(pred_arg, orig_label.argmax(dim=1, keepdim=False))\n",
    "    \n",
    "f1_accu = f1.calc()\n",
    "print(f1_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_1_4]",
   "language": "python",
   "name": "conda-env-torch_1_4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
