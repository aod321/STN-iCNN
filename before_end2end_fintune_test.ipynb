{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model import Stage2Model, FaceModel, SelectNet_resnet, SelectNet\n",
    "from helper_funcs import affine_crop, stage2_pred_softmax, calc_centroid, affine_mapback\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class ModelEnd2End(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelEnd2End, self).__init__()\n",
    "        self.modelA = FaceModel()\n",
    "        self.stn_model = SelectNet_resnet()\n",
    "        self.modelC = Stage2Model()\n",
    "\n",
    "    def forward(self, x, orig, inference=True, orig_label=None):\n",
    "        N, L, H, W = orig.shape\n",
    "        if inference:\n",
    "            self.modelA.eval()\n",
    "            self.stn_model.eval()\n",
    "            self.modelC.eval()\n",
    "            stage1_pred = F.softmax(self.modelA(x), dim=1)\n",
    "            assert stage1_pred.shape == (N, 9, 128, 128)\n",
    "            theta = self.stn_model(stage1_pred)\n",
    "            parts, _ = affine_crop(orig, label=None, theta_in=theta, map_location=x.device)\n",
    "            stage2_pred = self.modelC(parts)\n",
    "            softmax_stage2 = stage2_pred_softmax(stage2_pred) \n",
    "            final_pred = affine_mapback(softmax_stage2, theta, x.device)\n",
    "            return final_pred, parts, softmax_stage2\n",
    "        \n",
    "        elif orig_label is not None:\n",
    "            stage1_pred = F.softmax(self.modelA(x), dim=1)\n",
    "            assert stage1_pred.shape == (N, 9, 128, 128)\n",
    "            theta = self.stn_model(F.relu(stage1_pred))\n",
    "            cens = calc_centroid(orig_label)\n",
    "            parts, parts_labels, theta = affine_crop(orig, orig_label, points=cens, map_location=x.device, floor=False)\n",
    "            stage2_pred = self.modelC(parts)\n",
    "            softmax_stage2 = stage2_pred_softmax(stage2_pred)\n",
    "            final_pred = affine_mapback(softmax_stage2, theta, x.device)\n",
    "            return final_pred, stage2_pred, parts, parts_labels\n",
    "\n",
    "    def load_pretrain(self, path, device):\n",
    "        if len(path) == 0:\n",
    "            print(\"ERROR! No state path!\")\n",
    "            raise RuntimeError\n",
    "        elif len(path) == 1:\n",
    "            path = path[0]\n",
    "            print(\"load from\" + path)\n",
    "            state = torch.load(path, map_location=device)\n",
    "            self.modelA.load_state_dict(state['model1'])\n",
    "            self.stn_model.load_state_dict(state['select_net'])\n",
    "            self.modelC.load_state_dict(state['model2'])\n",
    "        elif len(path) == 2:\n",
    "            # AB, C\n",
    "            pathAB, pathC = path\n",
    "            print(\"load from\" + pathAB)\n",
    "            print(\"load from\" + pathC)\n",
    "            stateAB = torch.load(pathAB, map_location=device)\n",
    "            stateC = torch.load(pathC, map_location=device)\n",
    "            self.modelA.load_state_dict(stateAB['model1'])\n",
    "            self.stn_model.load_state_dict(stateAB['select_net'])\n",
    "            self.modelC.load_state_dict(stateC['model2'])\n",
    "        elif len(path) == 3:\n",
    "            # A, B, C\n",
    "            pathA, pathB, pathC = path\n",
    "            print(\"load from\" + pathA)\n",
    "            print(\"load from\" + pathB)\n",
    "            print(\"load from\" + pathC)\n",
    "            stateA = torch.load(pathA, map_location=device)\n",
    "            stateB = torch.load(pathB, map_location=device)\n",
    "            stateC = torch.load(pathC, map_location=device)\n",
    "            self.modelA.load_state_dict(stateA['model1'])\n",
    "            self.stn_model.load_state_dict(stateB['select_net'])\n",
    "            self.modelC.load_state_dict(stateC['model2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= ModelEnd2End()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from/home/yinzi/data4/STN-iCNN/checkpoints_AB_res/e0de5954/best.pth.tar\n",
      "load from/home/yinzi/data4/STN-iCNN/checkpoints_C/c1f2ab1a/best.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# path_AB = os.path.join(\"/home/yinzi/data4/STN-iCNN/checkpoints_AB_res/23903dfc\", 'best.pth.tar')\n",
    "path_AB = os.path.join(\"/home/yinzi/data4/STN-iCNN/checkpoints_AB_res/e0de5954\", 'best.pth.tar')\n",
    "\n",
    "path_C = os.path.join(\"/home/yinzi/data4/STN-iCNN/checkpoints_C/c1f2ab1a\", 'best.pth.tar')\n",
    "# path_C = os.path.join(\"/home/yinzi/data3/stn-new/checkpoints_C/9b41a676\", 'best.pth.tar')\n",
    "\n",
    "\n",
    "# c1f2ab1a best_error_all 0.245\n",
    "# python3 train_stage2.py  --batch_size 64 --cuda 6 --lr0 0.0008 --lr1 0.0008 --lr2 0.0008 --lr3 0.0008 --epochs 3000\n",
    "# 1 不增 + 4增\n",
    "model.eval()\n",
    "model.load_pretrain(path=[path_AB, path_C], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把两个 checkpoint存在一起\n",
    "state = {}\n",
    "fname = os.path.join(\"/home/yinzi/data4/STN-iCNN/\", 'before_end2end.pth.tar')\n",
    "state['model'] = model.state_dict()\n",
    "torch.save(state, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import HelenDataset\n",
    "from torchvision import transforms\n",
    "from preprocess import ToPILImage, ToTensor, OrigPad, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "testDataset = HelenDataset(txt_file='testing.txt',\n",
    "                           root_dir=\"/data1/yinzi/datas\",\n",
    "                           parts_root_dir=\"/home/yinzi/data3/recroped_parts\",\n",
    "                           transform=  transforms.Compose([\n",
    "                                    ToPILImage(),\n",
    "                                    Resize((128, 128)),\n",
    "                                    ToTensor(),\n",
    "                                    OrigPad()\n",
    "                                ])\n",
    "                           )\n",
    "dataloader = DataLoader(testDataset, batch_size=1,\n",
    "                            shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不考虑 AB 模型时(即使用groundtruth 指导切割)的F1 overall为0.9104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9104196111000484\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn.functional as F\n",
    "from helper_funcs import F1Accuracy\n",
    "f1 = F1Accuracy(num=9)\n",
    "model.eval()\n",
    "for iter,batch in enumerate(dataloader):\n",
    "    image, labels, orig, orig_label = batch['image'], batch['labels'], batch['orig'], batch['orig_label']\n",
    "    pred, stage2_pred, parts, parts_labels =model(image, orig=orig, inference=False, orig_label=orig_label)\n",
    "    pred_arg = pred.argmax(dim=1, keepdim=False).detach()\n",
    "#     plt.imshow(pred_arg[0])\n",
    "#     plt.pause(0.01)\n",
    "#     plt.imshow(orig_label.argmax(dim=1, keepdim=False)[0])\n",
    "#     plt.pause(0.01)\n",
    "    f1.collect(pred_arg, orig_label.argmax(dim=1, keepdim=False))\n",
    "    \n",
    "f1_accu = f1.calc()\n",
    "print(f1_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加上 AB 模型时(即采用预测信息指导切割)的F1 overall 为0.893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8928423572029238\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn.functional as F\n",
    "from helper_funcs import F1Accuracy\n",
    "f1 = F1Accuracy(num=9)\n",
    "model.eval()\n",
    "for iter,batch in enumerate(dataloader):\n",
    "    image, labels, orig, orig_label = batch['image'], batch['labels'], batch['orig'], batch['orig_label']\n",
    "    pred, _, _=model(image, orig=orig, inference=True)\n",
    "    pred_arg = pred.argmax(dim=1, keepdim=False).detach()\n",
    "#     plt.imshow(pred_arg[0])\n",
    "#     plt.pause(0.01)\n",
    "#     plt.imshow(orig_label.argmax(dim=1, keepdim=False)[0])\n",
    "#     plt.pause(0.01)\n",
    "    f1.collect(pred_arg, orig_label.argmax(dim=1, keepdim=False))\n",
    "    \n",
    "f1_accu = f1.calc()\n",
    "print(f1_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4707e9e24d5f0bae2539fa72abcaf774e8cede50280c96041108dbb07dd9cdc"
  },
  "kernelspec": {
   "display_name": "Python [conda env:torch_1_4]",
   "language": "python",
   "name": "conda-env-torch_1_4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
